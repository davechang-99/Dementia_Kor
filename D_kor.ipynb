{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMrN6ni7+GzYjl2NIsdiUuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davechang-99/Dementia_Kor/blob/main/D_kor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 설치 (Colab에서만 실행)\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 scikit-learn==1.3.2 transformers==4.40.1 librosa==0.10.1 matplotlib==3.7.1"
      ],
      "metadata": {
        "id": "eMLVy93z4XeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rmX5sB-E4a0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AozNIzBS3se8"
      },
      "outputs": [],
      "source": [
        "# 필수 패키지 임포트\n",
        "import os, random, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import librosa\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경변수 및 경로 설정\n",
        "SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "AUDIO_MAX_LEN = 16000*10  # 10초 기준 패딩\n",
        "N_MELS = 128\n",
        "DATA_DIR = '/content/drive/MyDrive/your_data_dir'  # 실제 데이터 위치로 변경\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "mj4pMznV4iFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비 및 경로/레이블 수집\n",
        "def get_audio_paths_and_labels(data_dir):\n",
        "    paths, labels = [], []\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_path):\n",
        "            for fname in os.listdir(label_path):\n",
        "                if fname.endswith('.wav'):\n",
        "                    paths.append(os.path.join(label_path, fname))\n",
        "                    labels.append(0 if label_dir == 'normal' else 1)  # normal=0, dementia=1\n",
        "    return paths, labels\n",
        "\n",
        "audio_paths, audio_labels = get_audio_paths_and_labels(DATA_DIR)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    audio_paths, audio_labels, test_size=0.2, random_state=SEED, stratify=audio_labels\n",
        ")\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, paths, labels, mode='cnn', feature_extractor=None, augment=False):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        y, sr = librosa.load(path, sr=16000)\n",
        "        if len(y) < AUDIO_MAX_LEN:\n",
        "            y = np.pad(y, (0, AUDIO_MAX_LEN - len(y)))\n",
        "        else:\n",
        "            y = y[:AUDIO_MAX_LEN]\n",
        "        if self.augment:\n",
        "            y = y + np.random.normal(0, 0.005, size=y.shape)\n",
        "        mel = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MELS)\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
        "        if self.mode == 'cnn':\n",
        "            return torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0), torch.tensor(label, dtype=torch.long)\n",
        "        elif self.mode == 'vit':\n",
        "            image = np.stack([mel_db]*3, axis=0)\n",
        "            if self.feature_extractor:\n",
        "                image = self.feature_extractor(images=image.transpose(1,2,0), return_tensors='pt')['pixel_values'][0]\n",
        "            else:\n",
        "                image = torch.tensor(image, dtype=torch.float32)\n",
        "            return image, torch.tensor(label, dtype=torch.long)\n",
        "        elif self.mode == 'raw':\n",
        "            return mel_db.flatten(), label\n"
      ],
      "metadata": {
        "id": "tCPP_pRt31yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의 (CNN, ViT, RandomForest)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_mels=N_MELS, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, (5,5), stride=2, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, (3,3), stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, (3,3), stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(64*(n_mels//8)*(int(AUDIO_MAX_LEN/512)), 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = x.flatten(1)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "UG_AbO1I311z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT: HuggingFace 모델 활용\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=2)"
      ],
      "metadata": {
        "id": "JHhPJZUr4nuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습, 평가, 시각화 함수 및 전체 실행\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    model.to(device)\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # 검증\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                outputs = model(x)\n",
        "                preds = outputs.argmax(1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        val_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Val Accuracy: {val_acc:.4f}\")\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "def eval_model(model, loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            outputs = model(x)\n",
        "            probs = nn.functional.softmax(outputs, dim=1)\n",
        "            preds = outputs.argmax(1).cpu().numpy()\n",
        "            y_true.extend(y.numpy())\n",
        "            y_pred.extend(preds)\n",
        "            y_prob.extend(probs.cpu().numpy())\n",
        "    return np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
        "\n",
        "def train_rf(train_dataset, val_dataset):\n",
        "    X_train = np.array([train_dataset[i][0] for i in range(len(train_dataset))])\n",
        "    y_train = np.array([train_dataset[i][1] for i in range(len(train_dataset))])\n",
        "    X_val = np.array([val_dataset[i][0] for i in range(len(val_dataset))])\n",
        "    y_val = np.array([val_dataset[i][1] for i in range(len(val_dataset))])\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_val)\n",
        "    y_prob = rf.predict_proba(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    auc = roc_auc_score(y_val, y_prob[:,1])\n",
        "    print(f\"RF ValAcc={acc:.4f}, ROC-AUC={auc:.4f}\")\n",
        "    return y_val, y_pred, y_prob\n",
        "\n",
        "def plot_results(y_true, y_prob, title):\n",
        "    if len(y_true) == 0 or len(y_prob) == 0:\n",
        "        print(\"No data for plotting.\")\n",
        "        return\n",
        "    from sklearn.metrics import roc_curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob[:,1])\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'{title} ROC curve (AUC={roc_auc_score(y_true, y_prob[:,1]):.2f}')\n",
        "    plt.plot([0,1],[0,1],'--',color='gray')\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(title); plt.legend(); plt.show()"
      ],
      "metadata": {
        "id": "6CUr97Ux314r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "train_cnn_ds = AudioDataset(train_paths, train_labels, mode='cnn', augment=True)\n",
        "val_cnn_ds = AudioDataset(val_paths, val_labels, mode='cnn')\n",
        "train_cnn_loader = DataLoader(train_cnn_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_cnn_loader = DataLoader(val_cnn_ds, batch_size=BATCH_SIZE)\n",
        "cnn_model = SimpleCNN(n_mels=N_MELS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "train_model(cnn_model, train_cnn_loader, val_cnn_loader, criterion, optimizer, EPOCHS, DEVICE)\n",
        "y_true_cnn, y_pred_cnn, y_prob_cnn = eval_model(cnn_model, val_cnn_loader, DEVICE)\n",
        "plot_results(y_true_cnn, y_prob_cnn, \"CNN\")"
      ],
      "metadata": {
        "id": "Fucw4Vz84uBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT\n",
        "train_vit_ds = AudioDataset(train_paths, train_labels, mode='vit', feature_extractor=feature_extractor, augment=True)\n",
        "val_vit_ds = AudioDataset(val_paths, val_labels, mode='vit', feature_extractor=feature_extractor)\n",
        "train_vit_loader = DataLoader(train_vit_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_vit_loader = DataLoader(val_vit_ds, batch_size=BATCH_SIZE)\n",
        "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "train_model(vit_model, train_vit_loader, val_vit_loader, criterion, optimizer_vit, EPOCHS, DEVICE)\n",
        "y_true_vit, y_pred_vit, y_prob_vit = eval_model(vit_model, val_vit_loader, DEVICE)\n",
        "plot_results(y_true_vit, y_prob_vit, \"ViT\")"
      ],
      "metadata": {
        "id": "tM7pwFk64t6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest\n",
        "train_rf_ds = AudioDataset(train_paths, train_labels, mode='raw', augment=True)\n",
        "val_rf_ds = AudioDataset(val_paths, val_labels, mode='raw')\n",
        "y_true_rf, y_pred_rf, y_prob_rf = train_rf(train_rf_ds, val_rf_ds)\n",
        "plot_results(y_true_rf, y_prob_rf, \"RandomForest\")"
      ],
      "metadata": {
        "id": "sDpX3ZCh4txN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 성능 비교 리포트\n",
        "def print_report(name, y_true, y_pred):\n",
        "    print(f\"\\n{name} Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
        "\n",
        "print_report(\"CNN\", y_true_cnn, y_pred_cnn)\n",
        "print_report(\"ViT\", y_true_vit, y_pred_vit)\n",
        "print_report(\"RandomForest\", y_true_rf, y_pred_rf)"
      ],
      "metadata": {
        "id": "0LJ5tnol317d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TEVxc4aq31-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UerC0cWT32BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-uHIwsr32D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZgkte8Q32Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pec2xUEu32Kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}